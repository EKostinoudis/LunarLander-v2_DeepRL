{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64cd89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\"\n",
    "\n",
    "# seeds\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abba0b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, num_states, num_actions, activation=nn.Tanh):\n",
    "        super().__init__()\n",
    "        self.actor = nn.Sequential(\n",
    "                        nn.Linear(num_states, 64),\n",
    "                        activation(),\n",
    "                        nn.Linear(64, 64),\n",
    "                        activation(),\n",
    "                        nn.Linear(64, num_actions),\n",
    "                        nn.Softmax(dim=-1),\n",
    "                        )\n",
    "        self.critic = nn.Sequential(\n",
    "                        nn.Linear(num_states, 64),\n",
    "                        activation(),\n",
    "                        nn.Linear(64, 64),\n",
    "                        activation(),\n",
    "                        nn.Linear(64, 1),\n",
    "                        )\n",
    "    \n",
    "    def action_prob(self, x):\n",
    "        '''\n",
    "        Calculate the actions, and return the actions and the log probalilities\n",
    "        '''\n",
    "        probs = self.actor(x)\n",
    "        dist = Categorical(probs)\n",
    "        actions = dist.sample() \n",
    "        return actions, dist.log_prob(actions)\n",
    "    \n",
    "    def get_prob_and_entropy_from_action(self, x, actions):\n",
    "        '''\n",
    "        Return the log probabilities based on states (x) and actions\n",
    "        '''\n",
    "        probs = self.actor(x)\n",
    "        dist = Categorical(probs)\n",
    "        return dist.log_prob(actions), dist.entropy()\n",
    "    \n",
    "    def get_v(self, x):\n",
    "        '''\n",
    "        Return the critic value\n",
    "        '''\n",
    "        return self.critic(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b102160f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PARALLEL_AGENTS = 16\n",
    "MAX_TRAJECTORY = 1024\n",
    "BATCH_SIZE = 32\n",
    "GAMMA = 0.99\n",
    "EPOCHS = 5\n",
    "V_LOSS_FACTOR = 0.5\n",
    "H_LOSS_FACTOR = 0.01\n",
    "EPSILON = 0.2\n",
    "\n",
    "SEED = 69 # for gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6459b3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the environment\n",
    "env = gym.vector.make(\"LunarLander-v2\", num_envs=PARALLEL_AGENTS)\n",
    "states, _ = env.reset(seed=SEED)\n",
    "num_states = states.shape[1]\n",
    "num_actions = env.action_space[0].n\n",
    "\n",
    "# initialize the model\n",
    "policy = ActorCritic(num_states, num_actions).to(device)\n",
    "\n",
    "optimizer = optim.Adam([\n",
    "    {\"params\": policy.actor.parameters(), 'lr': 1e-3},\n",
    "    {\"params\": policy.critic.parameters(), 'lr': 1e-3},\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2632162",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clip_loss(pt, advantage, epsilon):\n",
    "    # Calculates the clip loss\n",
    "    return torch.mean(torch.min(advantage * torch.clip(pt, min=1-epsilon, max=1+epsilon),\n",
    "                                advantage * pt\n",
    "                               )\n",
    "                     )\n",
    "\n",
    "    \n",
    "for episode in range(200):\n",
    "    states, _ = env.reset(seed=SEED)\n",
    "    \n",
    "    '''\n",
    "    Run the simulation to collect data.\n",
    "    \n",
    "    The vector environments autoreset sub-environments after they terminate or truncated,\n",
    "    so we don't need to reset all the environments.\n",
    "    '''\n",
    "    temp_episode_mem = []\n",
    "    rewards_sum = 0.\n",
    "    for t in range(MAX_TRAJECTORY):\n",
    "        # get next action\n",
    "        states = torch.tensor(states, dtype=torch.float32, device=device)\n",
    "        with torch.no_grad():\n",
    "            actions, log_probs = map(lambda x: x.cpu().numpy(), \n",
    "                                     policy.action_prob(states),\n",
    "                                    )\n",
    "        new_states, rewards, terminated, truncated, _ = env.step(actions)\n",
    "        done = torch.tensor([x or y for x,y in zip(terminated, truncated)], \n",
    "                            dtype=torch.float32, \n",
    "                            device=device).unsqueeze(-1)\n",
    "        temp_episode_mem.append((states, actions, rewards, new_states, log_probs, done))\n",
    "        \n",
    "        # hold the rewards for logging\n",
    "        rewards_sum += sum(rewards)\n",
    "        \n",
    "        # update the states\n",
    "        states = new_states\n",
    "            \n",
    "    '''\n",
    "    Prepare the data for training after the simulation was finished\n",
    "    '''\n",
    "    # hold the number of the completed and none-completed runs\n",
    "    completed_runs = 0\n",
    "    none_completed_runs = int(PARALLEL_AGENTS - torch.sum(temp_episode_mem[-1][-1]).item())\n",
    "    \n",
    "    # hold the sum of rewards of the completed runs\n",
    "    completed_rewards = 0.\n",
    "    completed_mask = temp_episode_mem[-1][-1]\n",
    "    training_batch = []\n",
    "    \n",
    "    new_states = torch.tensor(new_states, dtype=torch.float32, device=device)\n",
    "    with torch.no_grad():\n",
    "        v_target = policy.get_v(new_states)\n",
    "\n",
    "    for states, actions, rewards, new_states, log_probs, done in reversed(temp_episode_mem):\n",
    "        # reset v_target when the episode was finished\n",
    "        v_target = v_target * (1.-done)\n",
    "        \n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32, device=device).unsqueeze(-1)\n",
    "        v_target = rewards + v_target*GAMMA\n",
    "\n",
    "        with torch.no_grad():\n",
    "            v_omega = policy.get_v(states)\n",
    "\n",
    "        advantage = v_target - v_omega\n",
    "\n",
    "        # add the data to the training batch\n",
    "        training_batch.extend(zip(states, actions, rewards, log_probs, advantage, v_target))\n",
    "        \n",
    "        # log completed runs\n",
    "        completed_runs += int(torch.sum(done).item())\n",
    "    \n",
    "        # log the completed rewards\n",
    "        completed_mask = torch.logical_or(completed_mask, done)\n",
    "        completed_rewards += torch.sum(rewards * completed_mask).item()\n",
    "    \n",
    "    # save some memory\n",
    "    del temp_episode_mem\n",
    "    \n",
    "    '''\n",
    "    Train the model\n",
    "    '''\n",
    "    loss_list, loss_list_clip, loss_list_v, loss_list_h = [], [], [], []\n",
    "    for epoch in range(EPOCHS):\n",
    "        # shuffle the training data\n",
    "        random.shuffle(training_batch)\n",
    "        \n",
    "        for i in range(0, len(training_batch), BATCH_SIZE):\n",
    "            batch = training_batch[i:i+BATCH_SIZE]\n",
    "            states, actions, _, log_probs, advantage, v_target = zip(*batch)\n",
    "            \n",
    "            # to tensors\n",
    "            states = torch.vstack(states)\n",
    "            advantage = torch.vstack(advantage)\n",
    "            v_target = torch.vstack(v_target)\n",
    "            actions = torch.tensor(actions, dtype=torch.int8, device=device)\n",
    "            log_probs = torch.tensor(log_probs, dtype=torch.float32, device=device)\n",
    "            \n",
    "            # get the new log probabilities and the entropy values\n",
    "            log_probs_new, entropy = policy.get_prob_and_entropy_from_action(states, actions)\n",
    "            pt = torch.exp(log_probs_new - log_probs)\n",
    "            \n",
    "            # clip loss\n",
    "            clip_loss = get_clip_loss(pt, advantage, EPSILON)\n",
    "            \n",
    "            # loss for critic\n",
    "            v = policy.get_v(states)\n",
    "            v_loss = F.mse_loss(v, v_target)\n",
    "            \n",
    "            # entropy loss\n",
    "            h_loss = -torch.mean(entropy)\n",
    "            \n",
    "            # total loss\n",
    "            loss = -clip_loss + v_loss*V_LOSS_FACTOR - h_loss*H_LOSS_FACTOR\n",
    "            \n",
    "            # log the losses\n",
    "            loss_list.append(loss.item())\n",
    "            loss_list_clip.append(-clip_loss.item())\n",
    "            loss_list_v.append(v_loss.item()*V_LOSS_FACTOR)\n",
    "            loss_list_h.append(-h_loss.item()*H_LOSS_FACTOR)\n",
    "            \n",
    "            # update the networks\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    # print the results for the episode\n",
    "    if completed_runs == 0:\n",
    "        completed_rewards = float('nan')\n",
    "    else:\n",
    "        completed_rewards /= completed_runs\n",
    "    if episode == 0:\n",
    "        print(f\"{'Ep':>4}: {'Reward' :>8} | {'Comp re':>8} | {'loss':>9} | {'loss clip':>9} | {'loss v':>8} | {'loss h':>7} | {'Comp runs':>9} | {'None Comp':<9}\") \n",
    "    print(f'{episode:>4}: {rewards_sum/(completed_runs+none_completed_runs): 8.02f} | {completed_rewards: 8.02f} | {np.mean(loss_list): 9.02f} | {np.mean(loss_list_clip): 9.02f} | {np.mean(loss_list_v):8.02f} | {np.mean(loss_list_h):7.02f} | {completed_runs:>9} | {none_completed_runs:>9}') \n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71f617b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.vector.make(\"LunarLander-v2\", num_envs=PARALLEL_AGENTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f90fcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def save_plot(fig, name, folder=None):\n",
    "    '''Saves a figure'''\n",
    "    path = \"figures\"\n",
    "    if folder is not None:\n",
    "        path = os.path.join(path, folder)\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    fig.savefig(os.path.join(path, f\"{name}.png\"), bbox_inches='tight')\n",
    "\n",
    "def save_model(model, path, name):\n",
    "    '''Saves torch model'''\n",
    "    os.makedirs(path, exist_ok = True) \n",
    "    torch.save(model, os.path.join(path, name))\n",
    "\n",
    "save_model(policy, 'models', 'ppo_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133adc92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94569343",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.wrappers.monitoring import video_recorder\n",
    "import os\n",
    "\n",
    "path = 'videos'\n",
    "os.makedirs(path, exist_ok = True) \n",
    "\n",
    "# record video of the result\n",
    "env = gym.make(\"LunarLander-v2\", render_mode=\"rgb_array\")\n",
    "video = video_recorder.VideoRecorder(env, path=f'{path}/video_ppo.mp4')\n",
    "state, _ = env.reset(seed=SEED)\n",
    "while True:\n",
    "    video.capture_frame()\n",
    "\n",
    "    # next action\n",
    "    with torch.no_grad():\n",
    "        action = policy.action_prob(torch.tensor(state, device=device))[0].item()\n",
    "\n",
    "    state, _, terminated, truncated, _ = env.step(action)\n",
    "    if terminated or truncated: break\n",
    "video.close()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2ad5b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5672fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de05d968",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b9ef8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd89911",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clip_loss(pt, advantage, epsilon):\n",
    "    return torch.mean(torch.min(advantage * torch.clip(pt, min=1-epsilon, max=1+epsilon),\n",
    "                                advantage * pt\n",
    "                               )\n",
    "                     )\n",
    "\n",
    "    \n",
    "for episode in range(100):\n",
    "    states, _ = env.reset(seed=69)\n",
    "    \n",
    "    temp_episode_mem = []\n",
    "    training_batch = []\n",
    "    rewards_sum = 0.\n",
    "    for t in range(MAX_TRAJECTORY):\n",
    "        # get next action\n",
    "        states = torch.tensor(states, dtype=torch.float32, device=device)\n",
    "        with torch.no_grad():\n",
    "            actions, log_probs = map(lambda x: x.cpu().numpy(), \n",
    "                                     policy.action_prob(states),\n",
    "                                    )\n",
    "        new_states, rewards, terminated, truncated, _ = env.step(actions)\n",
    "        temp_episode_mem.append((states, actions, rewards, log_probs))\n",
    "        \n",
    "        # hold the rewards for logging\n",
    "        rewards_sum += sum(rewards)\n",
    "        \n",
    "        # check if any agent terminated or we are at the end of the episode\n",
    "        if terminated.any() or truncated.any() or t == MAX_TRAJECTORY-1:\n",
    "            done = torch.tensor([x or y for x,y in zip(terminated, truncated)], dtype=torch.float32, device=device).unsqueeze(-1)\n",
    "            new_states = torch.tensor(new_states, dtype=torch.float32, device=device)\n",
    "            with torch.no_grad():\n",
    "                v_target = policy.get_v(new_states)\n",
    "            v_target = v_target * (1.-done)\n",
    "            \n",
    "            for states, actions, rewards, log_probs in reversed(temp_episode_mem):\n",
    "                rewards = torch.tensor(rewards, dtype=torch.float32, device=device).unsqueeze(-1)\n",
    "                v_target = rewards + v_target*GAMMA\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    v_omega = policy.get_v(states)\n",
    "                \n",
    "                advantage = v_target - v_omega\n",
    "                \n",
    "                # add the data to the training batch\n",
    "                training_batch.extend(zip(states, actions, rewards, log_probs, advantage, v_target))\n",
    "                \n",
    "            # empty the list\n",
    "            temp_episode_mem = []\n",
    "            \n",
    "            # reset the environment\n",
    "            states, _ = env.reset(seed=69)\n",
    "        else:\n",
    "            states = new_states\n",
    "    \n",
    "    # Train the model\n",
    "    loss_list = []\n",
    "    loss_list_clip = []\n",
    "    loss_list_v = []\n",
    "    loss_list_h = []\n",
    "    for epoch in range(EPOCHS):\n",
    "        # shuffle the training data\n",
    "        random.shuffle(training_batch)\n",
    "        \n",
    "        for i in range(0, len(training_batch), BATCH_SIZE):\n",
    "            batch = training_batch[i:i+BATCH_SIZE]\n",
    "            states, actions, _, log_probs, advantage, v_target = zip(*batch)\n",
    "            \n",
    "            # to tensors\n",
    "            states = torch.vstack(states)\n",
    "            advantage = torch.vstack(advantage)\n",
    "            v_target = torch.vstack(v_target)\n",
    "            actions = torch.tensor(actions, dtype=torch.int8, device=device)\n",
    "            log_probs = torch.tensor(log_probs, dtype=torch.float32, device=device)\n",
    "            \n",
    "            # get the new log probabilities and the entropy values\n",
    "            log_probs_new, entropy = policy.get_prob_and_entropy_from_action(states, actions)\n",
    "            pt = torch.exp(log_probs_new - log_probs)\n",
    "            \n",
    "            # clip loss\n",
    "            clip_loss = get_clip_loss(pt, advantage, EPSILON)\n",
    "            \n",
    "            # loss for critic\n",
    "            v = policy.get_v(states)\n",
    "            v_loss = F.mse_loss(v, v_target)\n",
    "            \n",
    "            # entropy loss\n",
    "            h_loss = -torch.mean(entropy)\n",
    "            \n",
    "            loss = -clip_loss + v_loss*V_LOSS_FACTOR - h_loss*H_LOSS_FACTOR\n",
    "            loss_list.append(loss.item())\n",
    "            loss_list_clip.append(-clip_loss.item())\n",
    "            loss_list_v.append(v_loss.item()*V_LOSS_FACTOR)\n",
    "            loss_list_h.append(-h_loss.item()*H_LOSS_FACTOR)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    print(f'{episode:>4}: reward {rewards_sum/PARALLEL_AGENTS: 8.02f} | loss: {np.mean(loss_list): 8.02f} | loss clip: {np.mean(loss_list_clip): 8.02f} | loss v: {np.mean(loss_list_v): 8.02f} | loss h: {np.mean(loss_list_h): 7.02f}') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d92a4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c1e3b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e081e8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
